# Pseudocode for high-level functions

*** Add links (link adder) ***

01. find best getter (Premium > GVAIL > Direct)
02. unify link
03. link exists ?
  YES) mark and abort
  NO) continue
04. wait until can start check
05. check link
06. file exists ?
  NO) mark error and abort
  YES) continue
07. look for clones
08. clone exists ?
  NO) jump to 09
  1) add
  multiple) ask
09. autocommit and no clones ?
  YES) commit
  NO) ask to fix clones
10. commit ?
  YES) commit
  NO) discard
11. trigger downloads



*** Detect clone (link adder) ***

01. search database for files with
    db size_min <= this size_max and
    db size_max >= this size_min
	old: max |========| min
	new:   max |=======| min
02. search new list for the same thing
03. any links found ?
  NO) return
04. simplify guessed name
05. remove any links where simplified name doesn't match
06. return all links left


*** File Writer ***

# get file by id
my $file = new RSGet::Control::File $file_id;

# find best start position
my $pos1 = $file->startat();

## set curl start position at $pos1
## -> curl actually starts at $pos2

# get file writer	
my $out = new RSGet::Control::FilePart $file, $name, $size, $pos2, $stop;

# direct access to file no longer needed
undef $file;

$out->push( $data );


*** on data error (file writer) ***

1. start dumping all new data to DB
(in separate process)
2. find best fallback solution
3. move non-matching parts to another file (as specified in fallback)
4. restore dumped data drom DB and delete it
(in main process)
5. reopen files and allow writing directly


*** find best start position (file writer) ***

* to start new part all currently active parts must be at least 15 seconds old
  (so we can be more or less sure about their download speeds)
* save possible stop position where appropriate (e.g.
  MegaShares offers tickets, which limit ammount of downloaded
  data, so we know where we're going to stop)
1. Discard any chunks smaller than %{file_chunk_size_min}.
2. Mark dead areas (parts of file not being downloaded right now)
3. Is there any dead area?
  YES) Start %{file_chunk_size_shared} before first dead area, if there is one.
  NO) continue
4. Start in the middle of slowest active area, but only if it would take more
  than %{file_chunk_time_min} seconds to download it.


*** on data error, find best fallback (file writer) ***

1. check whether there are conflicts of data from the same URI
 YES) - this is veeeery bad
  2. file has multiple URIs ?
    YES) mark this one as unreliable, and disable it completely
    NO) - this is the only URI
     3. mark URI as "cannot continue"
     4. remove all data downloaded so far
     5. restart downloading at the beggining
 NO)
  2. group all parts by uri
  3. is one of those groups much smaller ?
    YES) move it to another file, and disable if not active
    NO) - keep checking
  4. is the other group inactive ?
    YES) move it to another file, and disable
    NO) - no more ideas, pick random one to remove ?
